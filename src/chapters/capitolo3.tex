\chapter{Models}
\label{cha:models}

Single-Image Super Resolution refers to the process of enhancing the size of small images while striving to minimize the reduction in quality or the loss of detail. Alternatively, it involves the task of reconstructing High Resolution (HR) images by utilizing the abundant information present in Low Resolution (LR) images, which lack finer details.

In other words, Image Super Resolution is about taking images that are of lower quality or resolution and applying techniques to enlarge them without significantly sacrificing their overall quality. This can be particularly useful in scenarios where higher resolution images are required for tasks like printing, display on high-resolution screens, or analyzing intricate details in images. The goal is to generate visually appealing results that closely resemble the appearance of true high-resolution images or at least contain improved details compared to the original low-resolution versions.

Neural networks are the perfect tool for this kind of task, from a set of images they are capable of capturing a lot of low-level image statistics and creating powerful high-order models able to deal with a very large degradation space.

The models have been chosen taking into account different characteristics. ESRGAN and Real-ESRGAN are two of the most popular model when it comes to Super Resolution, they share a similar structure and both use a GAN (Generative Adversal Network) architecture, on the other hand HAT-L and SwinIR are transformer-based models that take a new approach to vision tasks.

All models score highly in the ranking proposed by paperswithcode\cite{pwcode}, which provides a leveled field for comparing the latest models of the most common benchmark datasets.

The four models are implemented in the PyTorch-based open-source image and video restoration toolbox BasicSR (Basic Super Restoration). On top of offering the various models for SR it's equipped with useful logging capabilities and metrics for evaluation.


\section{ESRGAN}
\label{subsec:esrgan}

ESRGAN is based on the SRGAN\cite{ledig2017photorealistic} model. SRGAN, is a generative adversarial network (GAN) for image super-resolution (SR). The SRGAN model uses a perceptual loss function that consists of an adversarial loss and a content loss. The adversarial loss pushes the solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, the authors use a content loss motivated by perceptual similarity instead of similarity in pixel space. The actual networks consist mainly of residual blocks for feature extraction. The perceptual loss function is written as a weighted sum of a VGG content loss and an adversarial loss component. SRGAN is capable of inferring photo-realistic natural images for 4x upscaling factors and is the first framework to achieve this.

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.25]{figures/SRResNet.png}
  \caption{The basic architecture of SRResNet.}
  \label{img:SRResNet}
\end{figure}

ESRGAN stands for Enhanced SRGAN and introduces some major improvememnts to the framework. The core structure of the network employs an SRResNet architecture (Figure \ref{img:SRResNet}) with two notable differences: the lack of batch normalization and a new structure that replaces the original basic block with the proposed Residual-in-Residual Dense Block (RRDB), which combines multi-level residual network and dense
connections as depicted in Figure \ref{img:RRDB}

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.25]{figures/RRDB.png}
  \caption{Left: BN layers removed from residual blocks in SRGAN. Right: RRDB block used in the new deeper model.}
  \label{img:RRDB}
\end{figure}


\section{Real-ESRGAN}
\label{subsec:realesrgan}

Real-world degradations are usually too complex to be modeled with a simple combination of multiple degradations. Thus, models that assume an ideal bicubic
downsampling kernel or similar degradation processes will easily fail in real-world samples. Real-ESRGAN\cite{9711325} is an expansion of the powerful ESRGAN that introduces an high-order degradation modeling process to better simulate complex realworld degradations, in addition,
it employs a U-Net discriminator with spectral normalization to increase discriminator capability and stabilize the training dynamics.

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.4]{figures/UNET.png}
  \caption{Architecture of the U-Net discriminator.}
  \label{img:UNET}
\end{figure}

The authors claim that comparisons have shown its superior visual performance than prior works on various real datasets. While in this project the improved degradation model won't help much since our dataset already contains real-world degradations, the improved discriminator and better pratrained weights might yield better results compared to the previous methods.

\section{SwinIR}
\label{subsec:swinir}

CNN-based methods generally suffer from two basic
problems that stem from the basic convolution layer. First, the interactions between images and convolution kernels are content-independent. Using the same convolution kernel to restore different image regions may not be the best choice. Second, under the principle of local processing, convolution is not effective for long-range dependency modelling. As an alternative to CNN, Transformer designs a self-attention mechanism to capture global interactions between contexts and has shown promising performance in several vision problems. However, vision Transformers for image restoration usually divide the input image into patches with fixed size (e.g., 48Ã—48) and process each patch independently. Such a strategy inevitably gives rise to two drawbacks. First, border pixels cannot utilize neighbouring pixels that are out of the patch for image restoration. Second, the restored image may introduce border artifacts around each patch. While this problem can be alleviated by patch overlapping, it would introduce extra computational burden.

More recently Swin Transformer\cite{liu2021swin}  has shown great promise as it integrates the advantages of both CNN and
Transformer. SwinIR\cite{liang2021swinir} is a Swin Transformer based model that promises impressive performance on high-level vision tasks, such as Super Resolution and Denoising.

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.35]{figures/SwinIR.png}
  \caption{Architecture of the SwinIR model with its deep feature extraction module composed of residual Swin Transformer blocks (RSTB).}
  \label{img:swinir}
\end{figure}

\section{HAT-L}
\label{subsec:hatl}

HAT-L expands on the idea of Transformer-based methods for low-level vision tasks, such as image Super Resolution. The authurs propose a novel Hybrid Attention Transformer (HAT). It combines both channel attention and window based self-attention schemes \#TODO expand

\chapter{Training}
\label{cha:Training}

Most of the training has been done using Google Colab Pro on 16GB NVIDIA T4 and NVIDIA V100 GPUs. Due to memory constraints all models have been trained on the 64 to 256 dataset while only for some it's been possible to use the 128 to 512 dataset.
All training were run on pre-trained weights, fine-tuning the model until no improvements could be observed on loss and metrics.

\begin{itemize}
  \item weights for ESRGAN have been pretrained on  DIV2K\cite{Agustsson_2017_CVPR_Workshops}, Flickr2K\cite{Lim_2017_CVPR_Workshops} and OutdoorSceneTraining\cite{wang2018recovering}. It's been possible to fine tune the model with all our dataset: 64 to 256, 64 to 256 patches and 128 to 512. For each run it's been used a batch sizes of 4 and 2 and the memory allocated was on average around 7GB for the smaller dataset and 13GB for the bigger one.
  \item similarly to ESRGAN, Real-ESRGAN comes with weights pretrained on DIV2K, Flickr2K and OutdoorSceneTraining. The model have been trained on all datasets and, since they share the same generator architecture, the memory allocated was similar to the numbers of ESRGAN.
  \item SwinIR provides weights pretrained on DIV2K, just like the SRGAN-based models it's been trained on all the dataset configurations with batch sized of 4 and 2. The memory allocated was around 8GB for the smaller dataset and 14GB for the bigger dataset.
  \item HAT-L proved to be the heaviest of the models. It needed on average 15GB of VRAM just to run the smaller 64 to 256 dataset on a batch size of 2, for this reason it wasn't possible to try the larger 128 to 512 dataset. The pretrained weights provided with the model are trained with ImageNet\cite{deng2009imagenet}.
\end{itemize}

\section{Metrics}
\label{sec:metrics}

To evaluate the models performance we used the metrics provided and implemented directly by BasicSR:
\#TODO LPIPS metric?
\begin{itemize}
  \item \textbf{PSNR}\newline

  PSNR\cite{psnr} stands for "Peak Signal-to-Noise Ratio." It is a metric used to measure the quality of a reconstructed or compressed image or video in comparison to the original, uncompressed version. PSNR is widely used in the field of image and video processing to quantify the level of distortion introduced during compression or transmission.

  The PSNR value is calculated by comparing the maximum possible pixel value (usually 255 for an 8-bit image) to the mean squared error (MSE) between the original and the reconstructed/compressed image. The formula for calculating PSNR is as follows:
  \[
  \text{PSNR} = 10 \cdot \log_{10}\left(\frac{\text{MAX}^2}{\text{MSE}}\right)
  \]

  where \(MAX\) is the maximum possible pixel value and \(MSE\) is the mean squared error between the original and reconstructed images.


  \item \textbf{SSIM}\newline
  The structural similarity index measure (SSIM) is a method for predicting the perceived quality of digital images and videos. SSIM is used for measuring the similarity between two images. The SSIM index, just like PSNR, is a full reference metric; in other words, the measurement or prediction of image quality is based on an initial uncompressed or distortion-free image as reference. The measure between two windows \(x\) and \(y\) of common size \(N \times N\) is:


  \begin{equation}
    \SSIM(x,y) = \frac{(2\mu_x\mu_y + C_1) + (2 \sigma _{xy} + C_2)}
      {(\mu_x^2 + \mu_y^2+C_1) (\sigma_x^2 + \sigma_y^2+C_2)}
    \label{eq:SSMI}
  \end{equation}

  with:
  \begin{itemize}
    \itemsep0em
    \item \(y_x\) the pixel sample mean of \(x\)
    \item \(y_y\) the pixel sample mean of \(y\)
    \item \(\sigma_x^2\) the variance of \(x\)
    \item \(\sigma_y^2\) the variance of \(y\)
    \item \(\sigma _{xy}\) the cross-correlation between \(x\) and \(y\)
    \item \(C_1\) and \(C_2\) two variables to stabilize the division with weak denominator
  \end{itemize}
  \end{itemize}

A higher PSNR and SSIM value indicates better image quality, as it signifies a smaller amount of distortion between the original and the processed image. However, it's important to note that while PSNR and SSIM are widely used metrics, it does not always correlate perfectly with human perception of image quality. In some cases, images with higher values might still appear perceptually worse to human observers due to the limitations of the metric in capturing certain types of image distortions that humans are sensitive to.

\section{Training results}
\label{sec:training_res}

\subsection{ESRGAN}
\label{subsec:training_esrgan}

\begin{figure}[H]
  \centering
  \begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/ESRGAN_psnr.png}
    \label{fig:esrgan_psnr}
  \end{subfigure}%
  \begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/ESRGAN_ssim.png}
    \label{fig:esrgan_ssim}
  \end{subfigure}
  \caption{PSNR and SSIM for Real-ESRGAN}
  \label{fig:esrgan_metrics}
\end{figure}

\begin{figure}[H]
  \centering
  \begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/ESRGAN_ganloss.png}
    \label{fig:realesrgan_psnr}
  \end{subfigure}%
  \begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/ESRGAN_perceptual.png}
    \label{fig:realesrgan_ssim}
  \end{subfigure}
  \caption{GAN loss and perceptual loss for Real-ESRGAN}
  \label{fig:realesrgan_losses}
\end{figure}

\subsection{Real-ESRGAN}
\label{subsec:training_realesrgan}

\begin{figure}[H]
  \centering
  \begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/RealESRGAN_psnr.png}
    \label{fig:realesrgan_psnr}
  \end{subfigure}%
  \begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/RealESRGAN_ssim.png}
    \label{fig:realesrgan_ssim}
  \end{subfigure}
  \caption{PSNR and SSIM for Real-ESRGAN}
  \label{fig:realesrgan_metrics}
\end{figure}

\begin{figure}[H]
  \centering
  \begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/RealESRGAN_gan_loss.png}
    \label{fig:realesrgan_psnr}
  \end{subfigure}%
  \begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/RealESRGAN_perceptual.png}
    \label{fig:realesrgan_ssim}
  \end{subfigure}
  \caption{GAN loss and perceptual loss for Real-ESRGAN}
  \label{fig:realesrgan_losses}
\end{figure}

For Real-ESRGAN, even though the training for the smaller datasets was run to over 15k iterations for better visualization comparison it's been decided to cap Figure \ref{fig:realesrgan_metrics} at 6k iterations as, beyond that point, the model showed signs of overfit, with PSNR plateauing and SSIM decreasing substantially.

\subsection{SwinIR}
\label{subsec:training_swinir}
\begin{figure}[H]
  \centering
  \begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/swinir_psnr.png}
    \label{fig:swinir_psnr}
  \end{subfigure}%
  \begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/swinir_ssim.png}
    \label{fig:swinir_ssim}
  \end{subfigure}
  \caption{PSNR and SSIM for HAT-L}
  \label{fig:swinir_metrics}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.14]{figures/swinir_loss.png}
  \caption{Loss for HAT-L}
  \label{img:swinir_loss}
\end{figure}

\subsection{HAT-L}
\label{subsec:training_hatl}
\begin{figure}[H]
  \centering
  \begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/HATL_PSNR.png}
    \label{fig:hatl_psnr}
  \end{subfigure}%
  \begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/HATL_SSIM.png}
    \label{fig:hatl_ssim}
  \end{subfigure}
  \caption{PSNR and SSIM for HAT-L}
  \label{fig:hatl_metrics}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.14]{figures/HATL_loss.png}
  \caption{Loss for HAT-L}
  \label{img:hatl_loss}
\end{figure}
