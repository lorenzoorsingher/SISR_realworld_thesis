\chapter{Introduction}
\label{cha:introduction}

Cameras have become one of the most present kinds of technology in our everyday lives, they can be found in our smartphones, cars, doorbells, and a plethora of other appliances. For many use cases, high-end cameras are not employed, and the reasons can vary from space constraints to budget to bitrate limits. For example, applications such as security cameras oftentimes do not use high-resolution sensors and that can result in poor footage that lacks clarity and details. An increasingly used method to try and solve this problem is to enhance the image quality in post-processing using reconstruction techniques such as Super Resolution.

The objective of Single-Image Super Resolution (SISR) is to recreate high-resolution (HR) images from their low-resolution (LR) equivalent.
Deep convolutional networks have become a powerful tool for this kind of task, their ability to accurately recreate high-resolution details comes from training a rather large pool of example images.
Most existing methods generate their paired training set by artificially synthesizing LR directly from HR images. However, this dataset preparation strategy harms the application of these networks in real-world scenarios due to the inherent domain gap between the training and testing data \cite{9711325}.
This becomes very evident when the degradation of the LR images doesn't come from the common techniques used in the systemization of training datasets such as bicubic downsampling or JPEG compression but instead from other sources such as noise due to analog signal, and this is where I will focus.

Remotely controlled aerial vehicles can be difficult to maneuver at a distance from the ground and for this reason, they often require a camera, transmitting a live video feed to a ground station for navigation. While for slow-moving aerial platforms, a digital connection might be enough, as speed and distance increase the latency introduced by the conversion of the video signal to digital can become a serious problem, for this reason, many FPV (First-Person View) drones still equip low-resolution, low-latency analog cameras connected to a video transmitter (vTX) that streams directly to a video receiver (vRX) on the ground with a very small latency in the order of ten milliseconds \cite{oscar}. This video stream, albeit good for navigation, lacks quality and suffers from heavy noise, for this reason, these kinds of drones usually mount a high-resolution camera to capture the scenery. A second camera brings many downsides: reduced maneuverability, increased weight with a big impact on battery life and additional costs.

These cameras are usually mounted closely, in the same direction and focusing roughly on the same area, offering the chance to get image data synchronized in time in both high-resolution and low-resolution, enabling with some processing the creation of a dataset of paired LR and HR images, perfect for the training of a SISR model. With this in mind, it must be noted that a lot of work has gone into ensuring that the content of LR and HR frames matched as best as possible, although many factors contribute to the insurgence of discrepancies.

Four different models with different architectures have been chosen to be trained on the resulting dataset. All the models were among the best performing in super resolution tasks according to paperswithcode \cite{pwcode} scoring high both in PSNR\cite{psnr} (peak signal-to-noise ratio) and SSIM\cite{ssim} (structural similarity), these two were also the metrics used to check the validation performances of the models during training.

The first two models ESRGAN\cite{wang2018esrgan} and Real-ESRGAN\cite{wang2021realesrgan} are both GAN-based and, as the names suggest, share a lot of similarities, the latter being an extension of the original ESRGAN model with some additional improvements such a better and updated discriminator structure. While most state-of-the-art suer resolution models use deep convolutional neural networks the third and fourth models chosen for this project take a different approach making use of transformers, the SwinIR\cite{liang2021swinir} paper claim impressive performances in high-level vision tasks for their Swin Transformers\cite{liu2021swin}, which are an adaptation of language transformers to the vision domain. Another adaptation of transformers to vision tasks is the one used in the last model, HAT-L\cite{chen2023activating}, a newer entry that proposes a different structure claimed to be able to activate more pixels and get getter performances compared to SwinIR.

The research is divided into three chapters, in the first it will be explained in detail every step of the dataset creation, from the video capture to the whole post-processing procedure. The second chapter will be dedicated to the training process for the four models and the last one will be left for the final results and conclusions.
