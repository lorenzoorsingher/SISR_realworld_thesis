\newpage
\chapter{Introduction}
\label{cha:introduction}

Cameras have become one of the most present kinds of technology in our everyday lives, they can be found in our smartphones, cars, doorbells, and a plethora of other appliances. For many use cases, high-end cameras are not employed, and the reasons can vary from space constraints to budget to bitrate limits. For example, applications such as security cameras oftentimes do not use high-resolution sensors and that can result in poor footage that lacks clarity and details. An increasingly used method to try and solve this problem is to enhance the image quality in post-processing using reconstruction techniques such as Super Resolution.

The objective of Single-Image Super Resolution (SISR) is to recreate high-resolution (HR) images from their low-resolution (LR) equivalent.
Deep convolutional networks have become a powerful tool for this kind of task, thanks to their ability to accurately recreate high-resolution details from training on a large pool of example images.
Most existing methods generate their paired training set by artificially synthesizing LR directly from HR images. However, this dataset preparation strategy harms the application of these networks in real-world scenarios due to the inherent domain gap between the training and testing data \cite{9711325}.
This becomes very evident when the degradation of the LR images doesn't come from the common techniques used in the systemization of training datasets such as bicubic downsampling or JPEG compression but instead from other sources such as noise due to analog signal, and this is where I will focus.

Remotely controlled aerial vehicles can be difficult to maneuver at a distance from the ground and for this reason, they often require a camera, transmitting a live video feed to a ground station for navigation. While for slow-moving aerial platforms, a digital connection might be enough, as speed and distance increase the latency introduced by the conversion of the video signal to digital can become a serious problem, for this reason, many FPV (First-Person View) drones still equip low-resolution, low-latency analog cameras connected to a video transmitter (vTX) that streams directly to a video receiver (vRX) on the ground with a very small latency in the order of ten milliseconds \cite{oscar}. This video stream, albeit good for navigation, lacks quality and suffers from heavy noise, for this reason, these kinds of drones usually mount a high-resolution camera to capture the scenery. A second camera brings many downsides: reduced maneuverability, increased weight with a big impact on battery life and additional costs.

These cameras are usually mounted closely, in the same direction and focusing roughly on the same area, offering the chance to get image data synchronized in time in both high-resolution and low-resolution, enabling with some processing the creation of a dataset of paired LR and HR images, perfect for the training of a SISR model. With this in mind, it must be noted that a lot of work has gone into ensuring that the content of LR and HR frames matched as best as possible, even though many factors contribute to the insurgence of discrepancies.

The latter technique has been chosen to create our dataset, which was employed to train four different models with different architectures. All the models were among the best performing in super resolution tasks according to paperswithcode \cite{pwcode} scoring high both in PSNR \cite{psnr} (peak signal-to-noise ratio) and SSIM \cite{ssim} (structural similarity), these two were also the metrics used to check the validation performances of the models during training.

The first two models ESRGAN \cite{wang2018esrgan} and Real-ESRGAN \cite{wang2021realesrgan} are both GAN-based and, as the names suggest, share a lot of similarities, the latter being an extension of the original ESRGAN model with some additional improvements such a better and updated discriminator structure. While most state-of-the-art suer resolution models use deep convolutional neural networks the third and fourth models chosen for this project take a different approach making use of transformers, the SwinIR \cite{liang2021swinir} paper claim impressive performances in high-level vision tasks for their Swin Transformers \cite{liu2021swin}, which are an adaptation of language transformers to the vision domain. Another adaptation of transformers to vision tasks is the one used in the last model, HAT-L \cite{chen2023activating}, a newer entry that proposes a different structure claimed to be able to activate more pixels and get better performances compared to SwinIR.

This project has been engineered in collaboration with BlueTensor. BlueTensor is a company specialized in the development of AI-based solutions aimed at improving productivity and automating repetitive and time-consuming tasks by applying Computer Vision, NLP and Predictive Analysis, the help of their experts from the Computer Vision department has been key for this study.

The research is divided into six chapters. First is the summary of the project and the introduction. In the second one, it will be explained in detail every step of the dataset creation, from the video capture to the whole post-processing procedure. The third chapter will be dedicated to a briefing of the SR models used. The fourth chapter will explain the training procedure and the metrics used for evaluation. In the fifth chapter will be presented the final results of the project followed by our conclusions in the sixth.

